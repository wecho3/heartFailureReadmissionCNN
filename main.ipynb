{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\winsl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\winsl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# General Python Libraries\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import string\n",
    "\n",
    "# Pandas and Numpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# SKLearn\n",
    "import sklearn\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Download resources\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) MIMIC-III Data Processing\n",
    "The MIMIC-III data is publically available through <a href=\"url\" target=\"https://physionet.org/content/mimiciii/1.4/\">PhysioNet</a>. These are the relevant tables for this paper.\n",
    "* DIAGNOSIS_ICD table\n",
    "    * ROW_ID: (INT) Unique identifier for table\n",
    "    * SUBJECT_ID: (INT) Unique identifier for patient\n",
    "    * HADM_ID: (INT) Unique identifier for admission\n",
    "    * ICD9_CODE: (VARCHAR(10)) Final diagnosis associated to patient admission\n",
    "* ADMISSIONS table\n",
    "    * ROW_ID: (INT) Unique identifier for table\n",
    "    * SUBJECT_ID: (INT) Unique identifier for patient\n",
    "    * HADM_ID: (INT) Unique identifier for admission\n",
    "    * ADMITTIME: (TIMESTAMP(0)) Admit time for admission\n",
    "    * DISCHTIME: (TIMESTAMP(0)) Discharge time for admission\n",
    "* NOTEEVENTS table\n",
    "    * ROW_ID: (INT) Unique identifier for table\n",
    "    * SUBJECT_ID: (INT) Unique identifier for patient\n",
    "    * HADM_ID: (INT) Unique identifier for admission\n",
    "    * CATEGORY: (VARCHAR(50)) Type of note recorded ('Discharge summary' for discharge summary notes)\n",
    "    * DESCRIPTION: (VARCHAR(300)) 'Report' indicates a full note, 'Addendum' indicates additional text to be added to the previous report\n",
    "    * ISERROR: (CHAR(1)) '1' if physician indicates that the note is an error\n",
    "    * TEXT: (TEXT) Note text\n",
    "\n",
    "In addition, the paper references particular ICD-9 codes as related to heart failure:\n",
    "*398.91, 402.01, 402.11, 402.91, 404.01, 404.03, 404.11, 404.13, 404.91, 404.93, 428.0, 428.1, 428.20, 428.21, 428.22, 428.23, 428.30, 428.31, 428.32, 428.33, 428.40, 428.41, 428.42, 428.43, 428.9*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1) Loading MIMIC-III Data and ICD-9 HF Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total diagnoses for admissions:  651047\n",
      "Total admissions:  58976\n",
      "Total notes for admissions:  2083180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3457: DtypeWarning: Columns (4,5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "# File paths to raw data CSV (with gz compression)\n",
    "CURR_DIRNAME = os.getcwd()\n",
    "DATA_PATH_DIAGNOSES = os.path.join(CURR_DIRNAME, r'mimic-iii-clinical-database-1.4', r'DIAGNOSES_ICD.csv.gz')\n",
    "DATA_PATH_ADMISSIONS = os.path.join(CURR_DIRNAME, r'mimic-iii-clinical-database-1.4', r'ADMISSIONS.csv.gz')\n",
    "DATA_PATH_NOTES = os.path.join(CURR_DIRNAME, r'mimic-iii-clinical-database-1.4', r'NOTEEVENTS.csv.gz')\n",
    "\n",
    "# Record start time\n",
    "_START_RUNTIME = time.time()\n",
    "\n",
    "# Additional manual data\n",
    "HEART_FAILURE_ICD9 = {'39891', \n",
    "\t\t\t\t\t'40201', '40211', '40291', \n",
    "\t\t\t\t\t'40401', '40403', '40411', '40413', '40491', '40493', \n",
    "\t\t\t\t\t '4280',  '4281', '42820', '42821', '42822', '42823', '42830', '42831', '42832', '42833', '42840', '42841', '42842', '42843', '4289'}\n",
    "\n",
    "# Load CSV files\n",
    "# Load diagnoses data\n",
    "DATA_DIAGNOSES_RAW = pd.read_csv(DATA_PATH_DIAGNOSES, \n",
    "                                compression='gzip',\n",
    "                                on_bad_lines='skip')\n",
    "print(\"Total diagnoses for admissions: \", len(DATA_DIAGNOSES_RAW))\n",
    "# Load admissions data\n",
    "DATA_ADMISSIONS_RAW = pd.read_csv(DATA_PATH_ADMISSIONS, \n",
    "                                compression='gzip',\n",
    "                                on_bad_lines='skip')\n",
    "print(\"Total admissions: \", len(DATA_ADMISSIONS_RAW))\n",
    "# Load notes data\n",
    "DATA_NOTES_RAW = pd.read_csv(DATA_PATH_NOTES, \n",
    "                            compression='gzip',\n",
    "                            on_bad_lines='skip')\n",
    "print(\"Total notes for admissions: \", len(DATA_NOTES_RAW))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2) Retrieving All Heart Failure Admissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Admissions #:  14040\n",
      "<bound method NDFrame.head of         SUBJECT_ID  HADM_ID\n",
      "51             115   114585\n",
      "67             117   140784\n",
      "150            124   138376\n",
      "211            130   198214\n",
      "321             68   108329\n",
      "...            ...      ...\n",
      "650831       97132   144063\n",
      "650866       97144   109999\n",
      "650973       97172   133092\n",
      "650995       97488   152542\n",
      "651016       97488   161999\n",
      "\n",
      "[14040 rows x 2 columns]>\n"
     ]
    }
   ],
   "source": [
    "hf_admissions_filter = DATA_DIAGNOSES_RAW['ICD9_CODE'].map(lambda x: x in HEART_FAILURE_ICD9)\n",
    "HF_ADMISSIONS = DATA_DIAGNOSES_RAW[hf_admissions_filter][['SUBJECT_ID', 'HADM_ID']].drop_duplicates()\n",
    "\n",
    "print(\"All Admissions #: \", len(HF_ADMISSIONS))\n",
    "print(HF_ADMISSIONS.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3) Determining Readmissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1) Finding Admission Times\n",
    "The **ADMISSIONS** table contains the times of admit and discharge for each admission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF Admissions #:  14040\n",
      "HF Subjects #:  10436\n"
     ]
    }
   ],
   "source": [
    "# Find admission times (admit and discharge)\n",
    "admissions_wTimes = DATA_ADMISSIONS_RAW[['SUBJECT_ID', 'HADM_ID', 'ADMITTIME', 'DISCHTIME']].copy()\n",
    "admissions_wTimes.loc[:, 'DISCHTIME'] = admissions_wTimes['DISCHTIME'].map(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))\n",
    "admissions_wTimes.loc[:, 'ADMITTIME'] = admissions_wTimes['ADMITTIME'].map(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))\n",
    "hf_admissions_wTimes = HF_ADMISSIONS.merge(admissions_wTimes, how='left', on=['SUBJECT_ID', 'HADM_ID']).sort_values(by=['SUBJECT_ID', 'ADMITTIME'])\n",
    "hf_admissions_groupedBySubject = hf_admissions_wTimes.groupby('SUBJECT_ID')\n",
    "\n",
    "print(\"HF Admissions #: \", len(hf_admissions_wTimes))\n",
    "#print(hf_admissions_wTimes.head)\n",
    "print(\"HF Subjects #: \", len(hf_admissions_groupedBySubject))\n",
    "#print(hf_admissions_groupedBySubject.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2) Finding Time to Next Admission\n",
    "\n",
    "Time to next admission is determined by discharge time from current admission to the admit time of the subsequent admission. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF Admissions # w/ Readmit:  14040\n"
     ]
    }
   ],
   "source": [
    "# Find readmissions intervals\n",
    "hf_admissions_wTimes['NEXTADMITTIME'] = hf_admissions_groupedBySubject['ADMITTIME'].shift(-1, axis=0)\n",
    "get_days = lambda x: x.components.days if hasattr(x, \"components\") else float(\"NaN\")\n",
    "hf_admissions_wTimes['READMISSIONINTERVAL'] =  (hf_admissions_wTimes['NEXTADMITTIME'] - hf_admissions_wTimes['DISCHTIME']).map(get_days)\n",
    "\n",
    "print(\"HF Admissions # w/ Readmit: \", len(hf_admissions_wTimes))\n",
    "#print(hf_admissions_wTimes.head)\n",
    "#print(hf_admissions_wTimes.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2.A) General Readmissions\n",
    "General readmissions are defined as admissions that have a subsequent admission. The length of time to the next admission is irrelevant.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF General Readmissions #:  3604\n"
     ]
    }
   ],
   "source": [
    "# Find general readmissions\n",
    "HF_GEN_READMISSIONS = hf_admissions_wTimes.copy().dropna()\n",
    "\n",
    "print(\"HF General Readmissions #: \", len(HF_GEN_READMISSIONS))\n",
    "#print(HF_GEN_READMISSIONS.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2.B) 30 Day Readmissions\n",
    "30-day readmissions are defined as admissions that have a subsequent admission with an admit time that is within 30 days of the current admissions discharge time.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF 30-Day Readmissions #:  969\n"
     ]
    }
   ],
   "source": [
    "# Find 30-day readmissions\n",
    "HF_30DAY_READMISSIONS = HF_GEN_READMISSIONS.copy()[HF_GEN_READMISSIONS['READMISSIONINTERVAL'] <= 30]\n",
    "\n",
    "print(\"HF 30-Day Readmissions #: \", len(HF_30DAY_READMISSIONS))\n",
    "#print(HF_30DAY_READMISSIONS.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4) Determining Discharge Summary Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4.1) Loading Discharge Summary Notes\n",
    "The notes need to meet the following criteria for inclusion:\n",
    "* The note must not be marked as an error by a provider `(ISERROR != '1')`\n",
    "* The category is discharge summary `(CATEGORY == 'Discharge summary')`\n",
    "* Only full notes are included `(DESCRIPTION == 'Report')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total discharge summary notes w/o errors:  55177\n"
     ]
    }
   ],
   "source": [
    "# Find valid discharge summary notes\n",
    "# - Remove notes marked as error by physician\n",
    "# - Only keep notes marked as Discharge Summary\n",
    "dischargeSummary_notes_view = DATA_NOTES_RAW[['SUBJECT_ID', 'HADM_ID', 'CATEGORY', 'DESCRIPTION', 'ISERROR', 'TEXT']]\n",
    "\n",
    "dischargeSummary_notes_view = dischargeSummary_notes_view[(dischargeSummary_notes_view[\"ISERROR\"] != 1) & \n",
    "                                                          (dischargeSummary_notes_view[\"CATEGORY\"] == 'Discharge summary') &\n",
    "                                                          (dischargeSummary_notes_view[\"DESCRIPTION\"] == 'Report')]\n",
    "dischargeSummary_notes = dischargeSummary_notes_view[['SUBJECT_ID', 'HADM_ID', 'TEXT']].copy()\n",
    "\n",
    "print(\"Total discharge summary notes w/o errors: \", len(dischargeSummary_notes))\n",
    "#print(dischargeSummary_notes.head)\n",
    "#print(dischargeSummary_notes['TEXT'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4.2) Note Text Processing\n",
    "Note text is processed in the following manner:\n",
    "1. Tokenization through NLTK\n",
    "2. Removal of stopwords in NLTK corpora\n",
    "3. Removal of punctuation from string defintion list\n",
    "4. Removal of tokens that are numeric\n",
    "5. Removal of tokens that contain a number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of        SUBJECT_ID   HADM_ID                                               TEXT\n",
      "0           22532  167853.0  [admission, date, discharge, date, service, ad...\n",
      "1           13702  107527.0  [admission, date, discharge, date, date, birth...\n",
      "2           13702  167118.0  [admission, date, discharge, date, service, ca...\n",
      "3           13702  196489.0  [admission, date, discharge, date, service, me...\n",
      "4           26880  135453.0  [admission, date, discharge, date, date, birth...\n",
      "...           ...       ...                                                ...\n",
      "55970       43691  147266.0  [admission, date, discharge, date, date, birth...\n",
      "55971       80847  129802.0  [admission, date, discharge, date, date, birth...\n",
      "55972       41074  182558.0  [admission, date, discharge, date, date, birth...\n",
      "55973       76397  184741.0  [admission, date, discharge, date, date, birth...\n",
      "55974       87196  121964.0  [admission, date, discharge, date, date, birth...\n",
      "\n",
      "[55177 rows x 3 columns]>\n",
      "['admission', 'date', 'discharge', 'date', 'service', 'addendum', 'radiologic', 'studies', 'radiologic', 'studies', 'also', 'included', 'chest', 'ct', 'confirmed', 'cavitary', 'lesions', 'left', 'lung', 'apex', 'consistent', 'infectious', 'process/tuberculosis', 'also', 'moderate-sized', 'left', 'pleural', 'effusion', 'head', 'ct', 'head', 'ct', 'showed', 'intracranial', 'hemorrhage', 'mass', 'effect', 'old', 'infarction', 'consistent', 'past', 'medical', 'history', 'abdominal', 'ct', 'abdominal', 'ct', 'showed', 'lesions', 'sacrum', 'likely', 'secondary', 'osteoporosis', 'followed', 'repeat', 'imaging', 'outpatient', 'first', 'first', 'last', 'name', 'm.d', 'md', 'number', 'dictated', 'hospital', 'job', 'job', 'number']\n"
     ]
    }
   ],
   "source": [
    "# Create set of stopwords and punctuation to remove from token list\n",
    "stopwords_cache = set(stopwords.words(\"English\"))\n",
    "punctuation_cache = set(string.punctuation)\n",
    "remove_cache = stopwords_cache.union(punctuation_cache)\n",
    "\n",
    "# Tokenize note text and remove stopwords and numbers from note text\n",
    "dischargeSummary_notes_tokenized = dischargeSummary_notes.copy()\n",
    "dischargeSummary_notes_tokenized.loc[:, ['TEXT']] = dischargeSummary_notes_tokenized['TEXT'].map(\n",
    "    lambda x: [word.lower() for word in word_tokenize(x) if word.lower() not in remove_cache and not word.isdigit() and not any([token.isdigit() for token in word])])\n",
    "\n",
    "print(dischargeSummary_notes_tokenized.head)\n",
    "print(dischargeSummary_notes_tokenized['TEXT'][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4.3) Filtering for Longest Note per Admission\n",
    "The longest note in the admission is determined by the note text that contains the most tokens after processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total discharge summary notes w/ only longest:  52691\n",
      "<bound method NDFrame.head of        SUBJECT_ID   HADM_ID  \\\n",
      "17599       43126  124079.0   \n",
      "31434       42842  162017.0   \n",
      "51073       93321  115396.0   \n",
      "1519        66807  166588.0   \n",
      "54248       51821  197028.0   \n",
      "...           ...       ...   \n",
      "11081        3564  117638.0   \n",
      "12416        7995  190945.0   \n",
      "27470        6495  139808.0   \n",
      "36365         158  169433.0   \n",
      "20658       24855  156368.0   \n",
      "\n",
      "                                                    TEXT  NOTELENGTH  \n",
      "17599  [admission, date, discharge, date, date, birth...        4771  \n",
      "31434  [admission, date, discharge, date, date, birth...        4648  \n",
      "51073  [admission, date, discharge, date, date, birth...        4552  \n",
      "1519   [admission, date, discharge, date, date, birth...        4485  \n",
      "54248  [admission, date, discharge, date, date, birth...        4449  \n",
      "...                                                  ...         ...  \n",
      "11081  [admission, date, discharge, date, service, do...          50  \n",
      "12416  [admission, date, discharge, date, date, birth...          45  \n",
      "27470  [admission, date, discharge, date, date, birth...          39  \n",
      "36365  [admission, date, discharge, date, date, birth...          32  \n",
      "20658  [admission, date, discharge, date, service, hi...          29  \n",
      "\n",
      "[52691 rows x 4 columns]>\n"
     ]
    }
   ],
   "source": [
    "# Only keep largest note per admission\n",
    "dischargeSummary_notes_tokenized.loc[:, ['NOTELENGTH']] = dischargeSummary_notes_tokenized['TEXT'].map(lambda x: len(x))\n",
    "dischargeSummary_admissions = dischargeSummary_notes_tokenized.sort_values('NOTELENGTH', ascending=False).drop_duplicates(['SUBJECT_ID', 'HADM_ID'])\n",
    "\n",
    "print(\"Total discharge summary notes w/ only longest: \", len(dischargeSummary_admissions))\n",
    "print(dischargeSummary_admissions.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.5) Finding Heart Failure Admission with Discharge Summary Note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF Admissions w/ Notes #:  13746\n"
     ]
    }
   ],
   "source": [
    "# Find admissions w/ discharge summary notes\n",
    "hf_admissions_wNotes = HF_ADMISSIONS.merge(dischargeSummary_admissions, how='inner', on=['SUBJECT_ID', 'HADM_ID'])\n",
    "\n",
    "print(\"HF Admissions w/ Notes #: \", len(hf_admissions_wNotes))\n",
    "#print(hf_admissions_wNotes.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.6) Replacing Word Tokens with Word Embedding Vector\n",
    "Word embeddings are taken from the publically available Word2Vec model, <a href=\"url\" target=\"https://bio.nlplab.org/\">bio.nlplab.org</a>, trained on PubMed abstracts and PubMed Central full text articles. \n",
    "\n",
    "If the word is not found in the model, the word vector is randomly initialized.\n",
    "\n",
    "Each word embedding is of shape (200,)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained PubMed KeyedVectors through gensim\n",
    "pubmed_wv = KeyedVectors.load_word2vec_format(os.path.join(CURR_DIRNAME, r\"PubMed-and-PMC-w2v.bin\"), binary=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF Admissions w/ Notes WV #:  13746\n",
      "(1048, 200)\n"
     ]
    }
   ],
   "source": [
    "# Create replace words with word vectors from PubMed Word2Vec\n",
    "#   If word not present, initialize with random embeddings\n",
    "hf_wv_admissions = hf_admissions_wNotes.copy()\n",
    "hf_wv_admissions.loc[:, ['TEXT_EMBEDDING']] = hf_wv_admissions['TEXT'].map(lambda x: [pubmed_wv[word] if word in pubmed_wv else np.random.default_rng(12345).uniform(-1,1,(200,)) for word in x])\n",
    "\n",
    "print(\"HF Admissions w/ Notes WV #: \", len(hf_wv_admissions))\n",
    "print(np.array(hf_wv_admissions['TEXT_EMBEDDING'][0]).shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7) Positive Readmission Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF General Readmissions w/ Notes #:  3543\n",
      "HF 30-day Readmissions w/ Notes #:  962\n"
     ]
    }
   ],
   "source": [
    "# Find general readmissions w/ discharge summary notes\n",
    "HF_GEN_READMISSIONS_WNOTES = HF_GEN_READMISSIONS.merge(hf_wv_admissions, how='inner', on=['SUBJECT_ID', 'HADM_ID'])\n",
    "\n",
    "print(\"HF General Readmissions w/ Notes #: \", len(HF_GEN_READMISSIONS_WNOTES))\n",
    "#print(HF_GEN_READMISSIONS_WNOTES.head)\n",
    "\n",
    "# Find 30-day readmissions w/ discharge summary notes\n",
    "HF_30DAY_READMISSIONS_WNOTES = HF_30DAY_READMISSIONS.merge(hf_wv_admissions, how='inner', on=['SUBJECT_ID', 'HADM_ID'])\n",
    "\n",
    "print(\"HF 30-day Readmissions w/ Notes #: \", len(HF_30DAY_READMISSIONS_WNOTES))\n",
    "#print(HF_30DAY_READMISSIONS_WNOTES.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8) Negative Sampling\n",
    "Under-sampling is used to address the imbalance of positive to negative samples in the dataset. Negative samples are chosen in numbers matching the positive samples (general and 30-day readmissions) by selecting admissions without readmission and with discharge summary notes within the heart failure admission population. \n",
    "\n",
    "The goal of the model is to predict readmission within the heart failure population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF No General Readmissions w/ Notes #:  10203\n",
      "HF No 30-Day Readmissions w/ Notes #:  12784\n",
      "Sampled HF No General Readmissions w/ Notes #:  3543\n",
      "Sampled HF No 30-Day Readmissions w/ Notes #:  962\n"
     ]
    }
   ],
   "source": [
    "# Find heart failure admissions with discharge summary notes with no general readmissions\n",
    "positive_hf_readmissions = hf_wv_admissions.merge(HF_GEN_READMISSIONS_WNOTES, how='left', on=['SUBJECT_ID', 'HADM_ID'], indicator=True)\n",
    "negative_hf_readmissions = positive_hf_readmissions[positive_hf_readmissions['_merge'] == 'left_only'].copy()\n",
    "negative_hf_readmissions.rename(columns={\"TEXT_x\": \"TEXT\", \"NOTELENGTH_x\": \"NOTELENGTH\", \"TEXT_EMBEDDING_x\": \"TEXT_EMBEDDING\"}, inplace=True)\n",
    "\n",
    "print(\"HF No General Readmissions w/ Notes #: \", len(negative_hf_readmissions))\n",
    "\n",
    "# Find heart failure admissions with discharge summary notes with no 30-day readmissions\n",
    "positive_hf_30day_readmissions = hf_wv_admissions.merge(HF_30DAY_READMISSIONS_WNOTES, how='left', on=['SUBJECT_ID', 'HADM_ID'], indicator=True)\n",
    "negative_hf_30day_readmissions = positive_hf_30day_readmissions[positive_hf_30day_readmissions['_merge'] == 'left_only'].copy()\n",
    "negative_hf_30day_readmissions.rename(columns={\"TEXT_x\": \"TEXT\", \"NOTELENGTH_x\": \"NOTELENGTH\", \"TEXT_EMBEDDING_x\": \"TEXT_EMBEDDING\"}, inplace=True)\n",
    "\n",
    "print(\"HF No 30-Day Readmissions w/ Notes #: \", len(negative_hf_30day_readmissions))\n",
    "\n",
    "# Randomly sample from negative pool to match positive sample count\n",
    "readmission_gen_count = len(HF_GEN_READMISSIONS_WNOTES)\n",
    "no_readmission_gen_count = len(negative_hf_readmissions)\n",
    "negative_sample_list_gen = np.random.default_rng().choice(no_readmission_gen_count, (readmission_gen_count,), replace=False)\n",
    "HF_NO_GEN_READMISSIONS_WNOTES = negative_hf_readmissions.iloc[negative_sample_list_gen, :]\n",
    "print(\"Sampled HF No General Readmissions w/ Notes #: \", len(HF_NO_GEN_READMISSIONS_WNOTES))\n",
    "\n",
    "readmission_30day_count = len(HF_30DAY_READMISSIONS_WNOTES)\n",
    "no_readmission_30day_count = len(negative_hf_30day_readmissions)\n",
    "negative_sample_list_30day = np.random.default_rng().choice(no_readmission_30day_count, (readmission_30day_count,), replace=False)\n",
    "HF_NO_30DAY_READMISSIONS_WNOTES = negative_hf_30day_readmissions.iloc[negative_sample_list_30day, :]\n",
    "print(\"Sampled HF No 30-Day Readmissions w/ Notes #: \", len(HF_NO_30DAY_READMISSIONS_WNOTES))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.9) Save Processed Data to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HF_GEN_READMISSIONS_WNOTES' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4508/168418116.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Save final processed dataframes to file (pickle)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mHF_GEN_READMISSIONS_WNOTES\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'TEXT_EMBEDDING'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'NOTELENGTH'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'TEXT'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./positive_gen_readmissions.pkl.gz\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mHF_30DAY_READMISSIONS_WNOTES\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'TEXT_EMBEDDING'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'NOTELENGTH'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'TEXT'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./positive_30day_readmissions.pkl.gz\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mHF_NO_GEN_READMISSIONS_WNOTES\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'TEXT_EMBEDDING'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'NOTELENGTH'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'TEXT'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./negative_gen_readmissions.pkl.gz\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mHF_NO_30DAY_READMISSIONS_WNOTES\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'TEXT_EMBEDDING'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'NOTELENGTH'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'TEXT'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./negative_30day_readmissions.pkl.gz\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'HF_GEN_READMISSIONS_WNOTES' is not defined"
     ]
    }
   ],
   "source": [
    "# Save final processed dataframes to file (pickle)\n",
    "HF_GEN_READMISSIONS_WNOTES[['TEXT_EMBEDDING', 'NOTELENGTH', 'TEXT']].to_pickle(\"./positive_gen_readmissions.pkl.gz\")\n",
    "HF_30DAY_READMISSIONS_WNOTES[['TEXT_EMBEDDING', 'NOTELENGTH', 'TEXT']].to_pickle(\"./positive_30day_readmissions.pkl.gz\")\n",
    "HF_NO_GEN_READMISSIONS_WNOTES[['TEXT_EMBEDDING', 'NOTELENGTH', 'TEXT']].to_pickle(\"./negative_gen_readmissions.pkl.gz\")\n",
    "HF_NO_30DAY_READMISSIONS_WNOTES[['TEXT_EMBEDDING', 'NOTELENGTH', 'TEXT']].to_pickle(\"./negative_30day_readmissions.pkl.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Dataset Creation (TODO: Uncomment General Admission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed datasets\n",
    "#HF_GEN_READMISSIONS_WNOTES = pd.read_pickle(\"./positive_gen_readmissions.pkl.gz\", compression=\"gzip\")\n",
    "#HF_NO_GEN_READMISSIONS_WNOTES = pd.read_pickle(\"./negative_gen_readmissions.pkl.gz\", compression=\"gzip\")\n",
    "\n",
    "HF_30DAY_READMISSIONS_WNOTES = pd.read_pickle(\"./positive_30day_readmissions.pkl.gz\", compression=\"gzip\")\n",
    "HF_NO_30DAY_READMISSIONS_WNOTES = pd.read_pickle(\"./negative_30day_readmissions.pkl.gz\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1) Convert Dataframes to Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1) Combine positive and negative samples into one dataframe per type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30-Day Readmission:  962  +  962  =  1924\n"
     ]
    }
   ],
   "source": [
    "#readmission_gen_df = pd.concat([HF_GEN_READMISSIONS_WNOTES, HF_NO_GEN_READMISSIONS_WNOTES])\n",
    "readmission_30day_df = pd.concat([HF_30DAY_READMISSIONS_WNOTES, HF_NO_30DAY_READMISSIONS_WNOTES])\n",
    "\n",
    "#print(\"General Readmission: \", len(HF_GEN_READMISSIONS_WNOTES), \" + \", len(HF_NO_GEN_READMISSIONS_WNOTES), \" = \", len(readmission_gen_df))\n",
    "print(\"30-Day Readmission: \", len(HF_30DAY_READMISSIONS_WNOTES), \" + \", len(HF_NO_30DAY_READMISSIONS_WNOTES), \" = \", len(readmission_30day_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2) Find max note lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Note Length - 30-Day Readmissions:  3839\n"
     ]
    }
   ],
   "source": [
    "#max_words_gen_readmit = int(readmission_gen_df['NOTELENGTH'].max())\n",
    "max_words_30day_readmit = int(readmission_30day_df['NOTELENGTH'].max())\n",
    "\n",
    "#print(\"Max Note Length - General Readmissions: \", max_words_gen_readmit)\n",
    "print(\"Max Note Length - 30-Day Readmissions: \", max_words_30day_readmit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3) Pad note vectors to max length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1924, 3839, 200)\n"
     ]
    }
   ],
   "source": [
    "# Text Embedding Vectors\n",
    "#padded_note_wv_gen_readmit = pad_sequences(readmission_gen_df['TEXT_EMBEDDING'], maxlen=max_words_gen_readmit, padding=\"post\", value=0., dtype=np.float32)\n",
    "padded_note_wv_30day_readmit = pad_sequences(readmission_30day_df['TEXT_EMBEDDING'], maxlen=max_words_30day_readmit, padding=\"post\", value=0., dtype=np.float32)\n",
    "\n",
    "#print(padded_note_wv_gen_readmit.shape)\n",
    "print(padded_note_wv_30day_readmit.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4) Convert dataframes to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1924, 3839, 200])\n"
     ]
    }
   ],
   "source": [
    "# Text Embedding Vectors\n",
    "#readmission_gen_tensor = torch.tensor(padded_note_wv_gen_readmit, dtype=torch.float)\n",
    "readmission_30day_tensor = torch.tensor(padded_note_wv_30day_readmit, dtype=torch.float)\n",
    "\n",
    "#print(readmission_gen_tensor.shape)\n",
    "print(readmission_30day_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1924,)\n"
     ]
    }
   ],
   "source": [
    "# Text Token Vectors\n",
    "#readmission_gen_tensor_tokens = np.array(readmission_gen_df['TEXT'])\n",
    "readmission_30day_tensor_tokens = np.array(readmission_30day_df['TEXT'])\n",
    "\n",
    "#print(readmission_gen_tensor_tokens.shape)\n",
    "print(readmission_30day_tensor_tokens.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.5) Create labels for positive and negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1924])\n"
     ]
    }
   ],
   "source": [
    "# readmission_gen_labels = torch.cat(\n",
    "#     (\n",
    "#         torch.ones((len(HF_GEN_READMISSIONS_WNOTES),)), \n",
    "#         torch.zeros((len(HF_NO_GEN_READMISSIONS_WNOTES),))\n",
    "#     )\n",
    "# )\n",
    "readmission_30day_labels = torch.cat(\n",
    "    (\n",
    "        torch.ones((len(HF_30DAY_READMISSIONS_WNOTES),)), \n",
    "        torch.zeros((len(HF_NO_30DAY_READMISSIONS_WNOTES),))\n",
    "    )\n",
    ")\n",
    "\n",
    "#print(readmission_gen_labels.shape)\n",
    "print(readmission_30day_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2) Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, wv, labels, tokens):\n",
    "        \"\"\"\n",
    "        Store `seqs`. to `self.x` and `hfs` to `self.y`.\n",
    "        \"\"\"\n",
    "        self.x = wv\n",
    "        self.y = labels\n",
    "        self.tokens = tokens\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the number of samples (i.e. admissions).\n",
    "        \"\"\"\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Generates one sample of data.\n",
    "        \"\"\"\n",
    "        return self.x[index], self.y[index] \n",
    "\n",
    "    def getToken(self, index):\n",
    "        \"\"\"\n",
    "        Generate one sample token\n",
    "        \"\"\"\n",
    "        return self.tokens[index]\n",
    "\n",
    "# Create datasets for the general and 30-day readmissions\n",
    "#readmission_gen_dataset = CustomDataset(readmission_gen_tensor, readmission_gen_labels, readmission_gen_tensor_tokens)\n",
    "readmission_30day_dataset = CustomDataset(readmission_30day_tensor, readmission_30day_labels, readmission_30day_tensor_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3) Create Test and Training Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of 30-day train dataset: 1731\n",
      "Length of 30-day val dataset: 193\n"
     ]
    }
   ],
   "source": [
    "# Split is 90/10 for training/testing\n",
    "#split_gen_readmission = int(len(readmission_gen_dataset)*0.9)\n",
    "split_30day_readmission = int(len(readmission_30day_dataset)*0.9)\n",
    "\n",
    "# Create general readmission training and testing sets\n",
    "# lengths_gen_readmission = [split_gen_readmission, len(readmission_gen_dataset) - split_gen_readmission]\n",
    "# train_dataset_gen, val_dataset_gen = random_split(readmission_gen_dataset, lengths_gen_readmission)\n",
    "\n",
    "# print(\"Length of general train dataset:\", len(train_dataset_gen))\n",
    "# print(\"Length of general val dataset:\", len(val_dataset_gen))\n",
    "\n",
    "# Create 30-day readmission training and testing sets\n",
    "lengths_30day_readmission = [split_30day_readmission, len(readmission_30day_dataset) - split_30day_readmission]\n",
    "train_dataset_30day, val_dataset_30day = random_split(readmission_30day_dataset, lengths_30day_readmission)\n",
    "\n",
    "print(\"Length of 30-day train dataset:\", len(train_dataset_30day))\n",
    "print(\"Length of 30-day val dataset:\", len(val_dataset_30day))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4) Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define load data function for dataloader creation\n",
    "def load_data(train_dataset, val_dataset, collate_fn = None, batch_size = 32):\n",
    "    '''\n",
    "    Returns the data loader for train and validation dataset. \n",
    "    Set batchsize default to 32. \n",
    "    Set `shuffle=True` only for train dataloader.\n",
    "    \n",
    "    Arguments:\n",
    "        train dataset: train dataset of type `CustomDataset`\n",
    "        val dataset: validation dataset of type `CustomDataset`\n",
    "        collate_fn: collate function\n",
    "        batch_size: size of batches, default to 32\n",
    "        \n",
    "    Outputs:\n",
    "        train_loader, val_loader: train and validation dataloaders\n",
    "    '''\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True,\n",
    "                              collate_fn=collate_fn)\n",
    "    \n",
    "    val_loader = DataLoader(val_dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=False,\n",
    "                            collate_fn=collate_fn)\n",
    "    \n",
    "    return train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders for datasets\n",
    "#train_loader_gen, val_loader_gen = load_data(train_dataset_gen, val_dataset_gen)\n",
    "train_loader_30day, val_loader_30day = load_data(train_dataset_30day, val_dataset_30day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1) Mask Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2) Build CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoteCNN(nn.Module):\n",
    "    def __init__(self, filter_count=100):\n",
    "        super(NoteCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, filter_count, kernel_size = (1, 200), stride = 1)\n",
    "        self.conv2 = nn.Conv2d(1, filter_count, kernel_size = (2, 200), stride = 1, padding=(1, 0))\n",
    "        self.conv3 = nn.Conv2d(1, filter_count, kernel_size = (3, 200), stride = 1, padding=(2, 0))\n",
    "        self.fc = nn.Linear(filter_count * 3, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Create axis for one channel input\n",
    "        x = torch.unsqueeze(x, 1)\n",
    "        # Use convolution layers to create filters for each filter size\n",
    "        x_1 = self.conv1(x)\n",
    "        x_2 = self.conv2(x)\n",
    "        x_3 = self.conv3(x)\n",
    "        # Find maximum value across filters (max pool)\n",
    "        x_max_1, _ = torch.max(x_1, dim=2)\n",
    "        x_max_2, _ = torch.max(x_2, dim=2)\n",
    "        x_max_3, _ = torch.max(x_3, dim=2)\n",
    "        # Remove extra dimension\n",
    "        x_max_1 = torch.squeeze(x_max_1, dim=2)\n",
    "        x_max_2 = torch.squeeze(x_max_2, dim=2)\n",
    "        x_max_3 = torch.squeeze(x_max_3, dim=2)\n",
    "        # Combine filters together\n",
    "        x_layers = torch.cat([x_max_1, x_max_2, x_max_3], dim=1)\n",
    "        # Fully connected layer to find most prediction\n",
    "        pred = F.softmax(self.fc(F.relu(x_layers)), dim=1)\n",
    "        return pred\n",
    "\n",
    "# Initialize model\n",
    "model = NoteCNN()#.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NoteCNN size in GB: 0.000483608\n",
      "NoteCNN parameters:  120902\n"
     ]
    }
   ],
   "source": [
    "model_size = sum([param.nelement() * param.element_size() for param in model.parameters()]) / 1e9\n",
    "print(\"NoteCNN size in GB:\", model_size)\n",
    "model_parameters_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"NoteCNN parameters: \", model_parameters_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3) Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1) Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "\n",
    "CRITERION = nn.CrossEntropyLoss()\n",
    "OPTIMIZER = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2) Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, dataloader):\n",
    "    \"\"\"\n",
    "    Evaluate the model using data in data loader\n",
    "    :return:\n",
    "        Y_pred: prediction of model on the dataloder.\n",
    "            Should be an 2D numpy float array where the second dimension has length 2.\n",
    "        Y_test: truth labels. Should be an numpy array of ints\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    Y_pred = []\n",
    "    Y_test = []\n",
    "    for data, target in dataloader:\n",
    "        output = model(data)\n",
    "        _, prediction = torch.max(output, 1)\n",
    "        \n",
    "        Y_pred.append(prediction)\n",
    "        Y_test.append(target.long())\n",
    "        \n",
    "    Y_pred = np.concatenate(Y_pred, axis=0)\n",
    "    Y_test = np.concatenate(Y_test, axis=0)\n",
    "\n",
    "    return Y_pred, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3) Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 20\n",
    "\n",
    "def train_model(model, train_dataloader, n_epoch=N_EPOCHS, optimizer=OPTIMIZER, criterion=CRITERION):\n",
    "    \"\"\"\n",
    "    :param model: A CNN model\n",
    "    :param train_dataloader: the DataLoader of the training data\n",
    "    :param n_epoch: number of epochs to train\n",
    "    :return:\n",
    "        model: trained model\n",
    "    \"\"\"\n",
    "    model.train() # prep model for training\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        curr_epoch_loss = []\n",
    "        epoch_start_time = time.time()\n",
    "        for data, target in train_dataloader:\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            curr_epoch_loss.append(loss.cpu().data.numpy())\n",
    "        print(f\"Epoch {epoch}: curr_epoch_loss={np.mean(curr_epoch_loss)}\")\n",
    "        print(f\"Epoch {epoch} running time = {time.time() - epoch_start_time} seconds\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.4) Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: curr_epoch_loss=0.41994789242744446\n",
      "Epoch 0 running time = 37.83294653892517 seconds\n",
      "Epoch 1: curr_epoch_loss=0.39950308203697205\n",
      "Epoch 1 running time = 34.07367515563965 seconds\n",
      "Epoch 2: curr_epoch_loss=0.3843808174133301\n",
      "Epoch 2 running time = 36.448219776153564 seconds\n",
      "Epoch 3: curr_epoch_loss=0.37132877111434937\n",
      "Epoch 3 running time = 36.09221053123474 seconds\n",
      "Epoch 4: curr_epoch_loss=0.35690054297447205\n",
      "Epoch 4 running time = 34.35780715942383 seconds\n",
      "Epoch 5: curr_epoch_loss=0.34782901406288147\n",
      "Epoch 5 running time = 35.99550127983093 seconds\n",
      "Epoch 6: curr_epoch_loss=0.3393585681915283\n",
      "Epoch 6 running time = 41.21418642997742 seconds\n",
      "Epoch 7: curr_epoch_loss=0.3343295454978943\n",
      "Epoch 7 running time = 46.1887047290802 seconds\n",
      "Epoch 8: curr_epoch_loss=0.33052515983581543\n",
      "Epoch 8 running time = 39.929301500320435 seconds\n",
      "Epoch 9: curr_epoch_loss=0.3280082046985626\n",
      "Epoch 9 running time = 35.44194793701172 seconds\n",
      "Epoch 10: curr_epoch_loss=0.3257336914539337\n",
      "Epoch 10 running time = 42.720189571380615 seconds\n",
      "Epoch 11: curr_epoch_loss=0.3245716392993927\n",
      "Epoch 11 running time = 37.896509885787964 seconds\n",
      "Epoch 12: curr_epoch_loss=0.3227175772190094\n",
      "Epoch 12 running time = 39.605233669281006 seconds\n",
      "Epoch 13: curr_epoch_loss=0.32126009464263916\n",
      "Epoch 13 running time = 35.51256537437439 seconds\n",
      "Epoch 14: curr_epoch_loss=0.3202054500579834\n",
      "Epoch 14 running time = 38.190744400024414 seconds\n",
      "Epoch 15: curr_epoch_loss=0.3195534646511078\n",
      "Epoch 15 running time = 38.14941072463989 seconds\n",
      "Epoch 16: curr_epoch_loss=0.3189316391944885\n",
      "Epoch 16 running time = 34.84784650802612 seconds\n",
      "Epoch 17: curr_epoch_loss=0.31828248500823975\n",
      "Epoch 17 running time = 35.91351270675659 seconds\n",
      "Epoch 18: curr_epoch_loss=0.3179541826248169\n",
      "Epoch 18 running time = 38.47743773460388 seconds\n",
      "Epoch 19: curr_epoch_loss=0.3173977732658386\n",
      "Epoch 19 running time = 36.04639792442322 seconds\n"
     ]
    }
   ],
   "source": [
    "#model_general = train_model(model, train_loader_gen)\n",
    "model_30day = train_model(model, train_loader_30day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.5) Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Precision: 0.6632653061224489\n",
      "Validation Recall: 0.6701030927835051\n",
      "Validation F1: 0.6666666666666666\n",
      "Validation Accuracy: 0.6632124352331606\n"
     ]
    }
   ],
   "source": [
    "#y_pred_general, y_true_general = eval_model(model_general, val_loader_gen)\n",
    "\n",
    "# prec_general = precision_score(y_true_general, y_pred_general)\n",
    "# recall_general = recall_score(y_true_general, y_pred_general)\n",
    "# f1_general = f1_score(y_true_general, y_pred_general)\n",
    "# acc_general = accuracy_score(y_true_general, y_pred_general)\n",
    "\n",
    "# print((\"Validation Precision: \" + str(prec_general)))\n",
    "# print((\"Validation Recall: \" + str(recall_general)))\n",
    "# print((\"Validation F1: \" + str(f1_general)))\n",
    "# print((\"Validation Accuracy: \" + str(acc_general)))\n",
    "\n",
    "y_pred_30day, y_true_30day = eval_model(model_30day, val_loader_30day)\n",
    "\n",
    "prec_30day = precision_score(y_true_30day, y_pred_30day)\n",
    "recall_30day = recall_score(y_true_30day, y_pred_30day)\n",
    "f1_30day = f1_score(y_true_30day, y_pred_30day)\n",
    "acc_30day = accuracy_score(y_true_30day, y_pred_30day)\n",
    "\n",
    "print((\"Validation Precision: \" + str(prec_30day)))\n",
    "print((\"Validation Recall: \" + str(recall_30day)))\n",
    "print((\"Validation F1: \" + str(f1_30day)))\n",
    "print((\"Validation Accuracy: \" + str(acc_30day)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total running time = {:.2f} seconds\".format(time.time() - _START_RUNTIME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1) Datasets for Random Forest\n",
    "These datasets are the same as used for the CNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1) Load Datasets from Pre-Processed Files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed datasets\n",
    "#HF_GEN_READMISSIONS_WNOTES = pd.read_pickle(\"./positive_gen_readmissions.pkl.gz\", compression=\"gzip\")\n",
    "#HF_NO_GEN_READMISSIONS_WNOTES = pd.read_pickle(\"./negative_gen_readmissions.pkl.gz\", compression=\"gzip\")\n",
    "\n",
    "HF_30DAY_READMISSIONS_WNOTES = pd.read_pickle(\"./positive_30day_readmissions.pkl.gz\", compression=\"gzip\")\n",
    "HF_NO_30DAY_READMISSIONS_WNOTES = pd.read_pickle(\"./negative_30day_readmissions.pkl.gz\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2) Combine Positive and Negative Sample Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30-Day Readmission:  962  +  962  =  1924\n"
     ]
    }
   ],
   "source": [
    "#readmission_gen_df = pd.concat([HF_GEN_READMISSIONS_WNOTES, HF_NO_GEN_READMISSIONS_WNOTES])\n",
    "readmission_30day_df = pd.concat([HF_30DAY_READMISSIONS_WNOTES, HF_NO_30DAY_READMISSIONS_WNOTES])\n",
    "\n",
    "#print(\"General Readmission: \", len(HF_GEN_READMISSIONS_WNOTES), \" + \", len(HF_NO_GEN_READMISSIONS_WNOTES), \" = \", len(readmission_gen_df))\n",
    "print(\"30-Day Readmission: \", len(HF_30DAY_READMISSIONS_WNOTES), \" + \", len(HF_NO_30DAY_READMISSIONS_WNOTES), \" = \", len(readmission_30day_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3) Transform Dataframe into Numpy Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1924,)\n"
     ]
    }
   ],
   "source": [
    "# Text Token Vectors\n",
    "#readmission_gen_tensor_tokens = np.array(readmission_gen_df['TEXT'])\n",
    "readmission_30day_tensor_tokens = np.array(readmission_30day_df['TEXT'])\n",
    "\n",
    "#print(readmission_gen_tensor_tokens.shape)\n",
    "print(readmission_30day_tensor_tokens.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.4) Create Labels for Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1924])\n"
     ]
    }
   ],
   "source": [
    "# readmission_gen_labels = torch.cat(\n",
    "#     (\n",
    "#         torch.ones((len(HF_GEN_READMISSIONS_WNOTES),)), \n",
    "#         torch.zeros((len(HF_NO_GEN_READMISSIONS_WNOTES),))\n",
    "#     )\n",
    "# )\n",
    "readmission_30day_labels = torch.cat(\n",
    "    (\n",
    "        torch.ones((len(HF_30DAY_READMISSIONS_WNOTES),)), \n",
    "        torch.zeros((len(HF_NO_30DAY_READMISSIONS_WNOTES),))\n",
    "    )\n",
    ")\n",
    "\n",
    "#print(readmission_gen_labels.shape)\n",
    "print(readmission_30day_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2) Feature Weighting through Term Frequency - Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1) Setup Hyperparameters and Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FEATURES = [10000, 15000, 20000, 25000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2) Create Function to Use Pre-Tokenized Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_original(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3) Create TD-IDF Mapping for Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "readmission_30day_tfidf = {}\n",
    "for max_ft in MAX_FEATURES:\n",
    "    vectorizer = TfidfVectorizer(max_features=max_ft, tokenizer=return_original, preprocessor=return_original)\n",
    "    readmission_30day_tfidf[max_ft] = vectorizer.fit_transform(readmission_30day_tensor_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3) Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1) Create Training and Test Sets\n",
    "\n",
    "Use same split as CNN model. Features are TD-IDF weights of individual words instead of word vectors from pre-trained Word2Vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_datasets = {}\n",
    "for max_ft in readmission_30day_tfidf:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(readmission_30day_tfidf[max_ft], readmission_30day_labels, test_size=0.9, shuffle=True)\n",
    "    rf_datasets[max_ft] = (X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2) Define and Fit Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = {}\n",
    "for max_ft in MAX_FEATURES: \n",
    "    clf = RandomForestClassifier(max_depth=2, random_state=0, max_features=max_ft)\n",
    "    clf.fit(rf_datasets[max_ft][0], rf_datasets[max_ft][2])\n",
    "    y_pred[max_ft] = clf.predict(rf_datasets[max_ft][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3) Evaluate Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Features:  10000\n",
      "Validation Precision: 0.5668629100084104\n",
      "Validation Recall: 0.7883040935672515\n",
      "Validation F1: 0.6594911937377691\n",
      "Validation Accuracy: 0.5981524249422633\n",
      "Max Features:  15000\n",
      "Validation Precision: 0.5979667282809612\n",
      "Validation Recall: 0.7567251461988304\n",
      "Validation F1: 0.6680433660299432\n",
      "Validation Accuracy: 0.6287528868360277\n",
      "Max Features:  20000\n",
      "Validation Precision: 0.5833333333333334\n",
      "Validation Recall: 0.5995370370370371\n",
      "Validation F1: 0.591324200913242\n",
      "Validation Accuracy: 0.5866050808314087\n",
      "Max Features:  25000\n",
      "Validation Precision: 0.6170212765957447\n",
      "Validation Recall: 0.535796766743649\n",
      "Validation F1: 0.5735475896168108\n",
      "Validation Accuracy: 0.6016166281755196\n"
     ]
    }
   ],
   "source": [
    "for max_ft in MAX_FEATURES: \n",
    "    prec_30day = precision_score(rf_datasets[max_ft][3], y_pred[max_ft])\n",
    "    recall_30day = recall_score(rf_datasets[max_ft][3], y_pred[max_ft])\n",
    "    f1_30day = f1_score(rf_datasets[max_ft][3], y_pred[max_ft])\n",
    "    acc_30day = accuracy_score(rf_datasets[max_ft][3], y_pred[max_ft])\n",
    "\n",
    "    print(\"Max Features: \", max_ft)\n",
    "    print((\"Validation Precision: \" + str(prec_30day)))\n",
    "    print((\"Validation Recall: \" + str(recall_30day)))\n",
    "    print((\"Validation F1: \" + str(f1_30day)))\n",
    "    print((\"Validation Accuracy: \" + str(acc_30day)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Chi-squared based Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1) Select Correctly Identified Positive Samples from CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2) Find Top 20 Features from Correct Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = SelectKBest(chi2, k=20).fit_transform(X)\n",
    "print(X_new.shape)\n",
    "print(X_new)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
